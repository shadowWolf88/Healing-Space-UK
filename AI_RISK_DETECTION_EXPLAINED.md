# AI Integration with C-SSRS: Risk Detection During Therapy Chat

**Your Question:** "Will we be able to integrate the AI into this as well, so that if the user is in the 'chat' and showing signs of risk, this will be taken into account too?"

**Answer:** âœ… **YES - Fully integrated**

---

## ğŸ§  How the AI Detects Risk During Conversation

### Current Architecture

**Existing Therapy Chat API** (`/api/therapy/chat`)

The app already has:
- âœ… Patient sends message
- âœ… TherapistAI (Groq LLM) generates response
- âœ… Response is shown to patient

**What we're adding:**
- ğŸ†• SafetyMonitor analyzes the message for risk indicators
- ğŸ†• Returns risk_score (0-100) alongside therapy response
- ğŸ†• Frontend displays risk indicator (ğŸŸ¢/ğŸŸ /ğŸ”´)
- ğŸ†• If risk > threshold, prompt assessment

---

## ğŸ’¡ Example: Real Conversation with Risk Detection

### Chat Conversation

```
Patient: "Hi, I've been feeling quite down lately. Work has been 
          really stressful and I can't sleep properly."

AI Response: "I'm sorry to hear you're going through a tough time. 
             Stress and sleep issues often go together. Let's talk 
             about what's happening at work..."

[Backend Analysis]
- risk_score: 15 (LOW - mentions stress/sleep, no ideation)
- risk_level: low
- risk_indicators: ["sleep_problem", "stress"]
- action: none
- risk_indicator: ğŸŸ¢
```

### Risky Conversation

```
Patient: "I just don't see the point anymore. Nothing ever works 
          out for me. I think everyone would be better off without me."

AI Response: "I hear that you're in a lot of pain right now. The 
             feelings you're describing are serious, and I want to 
             help. Have you talked to your clinician about these 
             thoughts?"

[Backend Analysis]
- risk_score: 72 (HIGH - explicit ideation + hopelessness)
- risk_level: high
- risk_indicators: [
    "mentions_hopelessness",
    "expresses_worthlessness", 
    "others_better_without_me"
  ]
- action: offer_assessment
- risk_indicator: ğŸŸ 
- show_safety_banner: true
  
[Frontend Action]
- Display orange risk indicator in chat
- Show modal: "âš ï¸ We're concerned. Would you like to take 
             a safety assessment?"
- [Yes] â†’ Opens C-SSRS assessment
- [No] â†’ Continues monitoring
```

---

## ğŸ” Risk Language Detection

### What the AI Monitors For

**Tier 1: Suicidal Ideation (High Priority)**
```
"I want to kill myself"
"I've thought about ending it"
"I should just end this"
"Can't go on anymore"
"Everything would be better if I was dead"
"How do people kill themselves"
"I don't want to be alive"
```

**Tier 2: Planning/Intent (Very High Priority)**
```
"I have a plan"
"I know how I would do it"
"I've decided when"
"I've said goodbye to people"
"I've prepared"
"I'll do it after [event]"
```

**Tier 3: Hopelessness/Worthlessness (Medium Priority)**
```
"There's no point"
"Everything's hopeless"
"I'm worthless"
"I'm a burden"
"Nothing will get better"
"I'm alone"
```

**Tier 4: Behavioral Changes (Medium Priority)**
```
"Stopped taking meds"
"Giving away my things"
"Haven't eaten in days"
"Haven't left my room"
"Getting my affairs in order"
```

**Tier 5: Severe Distress (Low-Medium Priority)**
```
"Can't breathe"
"Chest pain"
"Panic attack"
"Losing control"
"Everything's spinning"
```

---

## ğŸ¤– Implementation in Backend

### Code Enhancement (api.py)

**Current code (~line 9548):**
```python
@app.route('/api/therapy/chat', methods=['POST'])
def therapy_chat():
    username = get_authenticated_username()
    if not username:
        return jsonify({'error': 'Unauthorized'}), 401
    
    message = request.json.get('message')
    
    # Generate AI response
    response = TherapistAI.generate_response(message)
    
    return jsonify({'response': response})
```

**Enhanced code:**
```python
@app.route('/api/therapy/chat', methods=['POST'])
def therapy_chat():
    username = get_authenticated_username()
    if not username:
        return jsonify({'error': 'Unauthorized'}), 401
    
    message = request.json.get('message')
    
    # 1. Generate AI therapy response
    ai_response = TherapistAI.generate_response(message)
    
    # 2. NEW: Analyze for risk indicators
    risk_analysis = SafetyMonitor.analyze_message(message)
    
    # 3. Log the message with risk metadata (NEVER store content)
    log_event(username, 'therapy', 'message_sent', 
              f"risk_score={risk_analysis['score']}")
    
    # 4. Check if alert needed
    alert_sent = False
    if risk_analysis['level'] in ['high', 'critical']:
        # Get patient's assigned clinician
        clinician = get_patient_clinician(username)
        if clinician:
            # Send alert email
            send_risk_alert_email(clinician, username, risk_analysis)
            alert_sent = True
            
            # Log alert
            log_event(username, 'therapy', 'risk_alert_sent',
                     f"level={risk_analysis['level']}")
    
    # 5. Return response with risk data
    return jsonify({
        'response': ai_response,
        'risk_score': risk_analysis['score'],        # 0-100
        'risk_level': risk_analysis['level'],        # low/moderate/high/critical
        'risk_indicators': risk_analysis['indicators'],
        'alert_sent': alert_sent,
        'suggested_action': risk_analysis['action']  # 'none'/'offer_assessment'/'require_assessment'
    })
```

---

## ğŸ§¬ SafetyMonitor Class (New)

**Location:** `api.py` or separate `safety_monitor.py`

```python
class SafetyMonitor:
    """Monitors conversations for suicide risk indicators"""
    
    RISK_KEYWORDS = {
        'ideation': [
            'kill myself', 'end my life', 'don\'t want to live',
            'want to die', 'better off dead', 'no point', 'can\'t go on'
        ],
        'planning': [
            'have a plan', 'know how', 'decided when', 'prepared',
            'goodbye', 'last time', 'final'
        ],
        'hopelessness': [
            'hopeless', 'pointless', 'nothing works', 'worthless',
            'burden', 'failure', 'give up'
        ],
        'behavior': [
            'stopped meds', 'giving away', 'affairs in order',
            'haven\'t eaten', 'sleeping too much'
        ]
    }
    
    RISK_WEIGHTS = {
        'ideation': 30,      # High weight
        'planning': 40,      # Highest weight
        'hopelessness': 15,  # Medium weight
        'behavior': 10       # Lower weight
    }
    
    @staticmethod
    def analyze_message(message):
        """
        Analyze message for risk indicators
        
        Returns:
        {
            'score': 0-100,
            'level': 'low'/'moderate'/'high'/'critical',
            'indicators': ['list', 'of', 'detected', 'phrases'],
            'action': 'none'/'offer_assessment'/'require_assessment'
        }
        """
        
        message_lower = message.lower()
        score = 0
        indicators = []
        
        # Check each category
        for category, keywords in SafetyMonitor.RISK_KEYWORDS.items():
            weight = SafetyMonitor.RISK_WEIGHTS[category]
            
            for keyword in keywords:
                if keyword in message_lower:
                    score += weight
                    indicators.append(keyword)
        
        # Cap score at 100
        score = min(score, 100)
        
        # Determine risk level
        if score >= 70:
            level = 'critical'
            action = 'require_assessment'  # Force assessment
        elif score >= 50:
            level = 'high'
            action = 'offer_assessment'    # Prompt assessment
        elif score >= 25:
            level = 'moderate'
            action = 'none'                # Just monitor
        else:
            level = 'low'
            action = 'none'                # No action needed
        
        return {
            'score': score,
            'level': level,
            'indicators': list(set(indicators)),  # Remove duplicates
            'action': action
        }
```

---

## ğŸ’» Frontend Integration

### What Happens in Browser

```javascript
// In sendMessage() function (line 9548 of index.html)

const response = await fetch('/api/therapy/chat', {
    method: 'POST',
    headers: { 'Content-Type': 'application/json' },
    body: JSON.stringify({
        username: currentUser,
        message: message
    })
});

const data = await response.json();

// Display AI response
const aiMessageEl = addMessage(data.response, 'ai');

// NEW: Handle risk data
if (data.risk_score > 0) {
    // Update risk indicator color
    updateRiskIndicator(data.risk_level);
    
    // Log for tracking
    console.log(`Risk score: ${data.risk_score} (${data.risk_level})`);
    
    // If high risk, show prompt
    if (data.risk_level === 'high') {
        showRiskPrompt(data);  // Modal: "We're concerned..."
    }
    
    if (data.risk_level === 'critical') {
        showRiskPrompt(data);  // Modal with urgency
    }
}
```

### Risk Indicator Display

**In chat area (subtle):**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Chat messages...]             â”‚
â”‚                                â”‚
â”‚ ğŸŸ¢ Low Risk                    â”‚ â† Updates in real-time
â”‚                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Color changes based on conversation:**
- ğŸŸ¢ Green (Low) â†’ Normal chat
- ğŸŸ¡ Yellow (Moderate) â†’ Some concerns
- ğŸŸ  Orange (High) â†’ Significant risk
- ğŸ”´ Red (Critical) â†’ Immediate danger (with pulse animation)

---

## ğŸ“± User Experience: Risk Detected

### Scenario: Patient Mentions Suicidal Thoughts

```
Patient Types: "I can't handle this anymore. Sometimes I think 
              about just ending it all, you know?"

[Frontend sends message]
[Waiting... "AI Thinking..."]

[Backend analyzes]
- "ending it all" detected (planning indicator)
- "can't handle this" detected (hopelessness indicator)
- risk_score calculated: 65 (HIGH)

[Frontend receives response]
{
    "response": "I hear that you're in a lot of pain right now. 
                What you're describing is serious, and I want to 
                help make sure you're safe...",
    "risk_score": 65,
    "risk_level": "high",
    "risk_indicators": ["ending it", "can't handle"],
    "alert_sent": true,
    "suggested_action": "offer_assessment"
}

[Frontend actions]
1. Display AI response
2. Change indicator from ğŸŸ¢ to ğŸŸ 
3. Show modal:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ âš ï¸ We're Concerned       â”‚
   â”‚                          â”‚
   â”‚ Your recent messages     â”‚
   â”‚ suggest you may be at    â”‚
   â”‚ risk. Would you like to  â”‚
   â”‚ complete a brief safety  â”‚
   â”‚ assessment?              â”‚
   â”‚                          â”‚
   â”‚ [Yes, Start]             â”‚
   â”‚ [Remind Later]           â”‚
   â”‚ [I'm Safe]               â”‚
   â”‚                          â”‚
   â”‚ Emergency: 999 | 116 123 â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
4. In background: Clinician receives email alert
```

---

## ğŸš¨ Escalation Protocol

### Score-Based Actions

```
Score: 0-20  (LOW)
â””â”€ No action
   Continue normal chat

Score: 21-40  (MODERATE)
â””â”€ Log for pattern detection
   Watch for trends
   No immediate alert

Score: 41-60  (HIGH)
â””â”€ Show assessment prompt (optional)
   Email clinician (info only)
   Monitor closely

Score: 61-80  (CRITICAL)
â””â”€ Show assessment prompt (urgent)
   Email clinician (high priority)
   Force safety plan if assessment shows HIGH/CRITICAL

Score: 81-100  (EMERGENCY)
â””â”€ Immediate clinician alert
   Suggest emergency services
   May require manual intervention
```

---

## ğŸ” Privacy Protection

### What's NOT Stored

âŒ Chat message content (for low/moderate risk)  
âŒ Full conversation text  
âŒ Recording of voice (discarded after transcription)  

### What IS Stored (GDPR Compliant)

âœ… Metadata only: timestamp, risk_score, indicators list  
âœ… Assessment responses (explicit C-SSRS answers)  
âœ… Clinician actions (what they did about alert)  
âœ… Patient consent for monitoring  

### Audit Trail

```
[2024-02-07 14:32:15] Patient john_doe sent message
[2024-02-07 14:32:16] Risk analysis: score=65, level=high
[2024-02-07 14:32:17] Alert sent to dr_smith@university.ac.uk
[2024-02-07 14:33:20] Patient offered assessment
[2024-02-07 14:35:00] Patient accepted assessment
[2024-02-07 14:38:45] Patient submitted C-SSRS (HIGH risk)
[2024-02-07 14:39:00] Safety plan form shown
[2024-02-07 14:45:30] Patient submitted safety plan
[2024-02-07 14:46:00] Clinician notified via alert email
```

---

## ğŸ¯ Benefits of This Approach

| Benefit | Why It Works |
|---------|------------|
| **Always On** | Risk detection happens automatically, patient doesn't have to remember |
| **Non-Invasive** | Doesn't interrupt flow, subtle indicator |
| **Contextual** | Assessment triggered when patient is actively engaging |
| **Clinician Ready** | Clinician gets alert immediately if HIGH/CRITICAL |
| **Evidence-Based** | Uses published C-SSRS, not just chat analysis |
| **Safe Fallback** | If AI misses something, clinician can still see pattern |
| **Compliant** | GDPR/NHS aligned, no full content storage |
| **Humane** | Respectful language, not accusatory |

---

## Summary

**Your original question:** "Can we integrate AI to detect risk during therapy chat?"

**Answer with this implementation:**

âœ… **Yes** - Every message analyzed for risk  
âœ… **Yes** - Risk score calculated (0-100)  
âœ… **Yes** - AI provides context AND safety response  
âœ… **Yes** - Clinician alerted if HIGH/CRITICAL  
âœ… **Yes** - Can trigger C-SSRS assessment automatically  
âœ… **Yes** - Privacy protected (metadata only storage)  
âœ… **Yes** - Works alongside formal assessments  

This creates a **comprehensive safety system** that combines:
1. **Passive monitoring** (AI detects during chat)
2. **Active assessment** (Patient takes C-SSRS when prompted or proactively)
3. **Clinician oversight** (Real-time alerts for action)
4. **Safety planning** (Automatic for HIGH/CRITICAL)

**All integrated seamlessly into the existing therapy chat interface.**

